{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Abhishek's\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abhishek's\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#downloading libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "#nltk.download()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten  \n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pprint\n",
    "\n",
    "reddit = praw.Reddit(client_id='300HIOocldVcKA', client_secret='PCktLUhpPaRB1RrBCouEDPdpEBU', user_agent='abhishek chopra') # potentially needs configuring, see docs\n",
    "solved_url = 'https://www.reddit.com/r/chelseafc/comments/cytgd2/just_met_our_number_9_in_barcelona_luckily_hes/'\n",
    "\n",
    "post = reddit.submission(url=solved_url)\n",
    "print(post.title) # this will fetch the lazy submission-object...\n",
    "pprint.pprint(vars(post)) # ... allowing you to list all available fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "posts = []\n",
    "india_subreddit = reddit.subreddit('India')\n",
    "posts=[]\n",
    "for post in india_subreddit.top('all',limit=1000):\n",
    "    posts.append([post.title, post.score, post.id, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n",
    "posts_pd = pd.DataFrame(posts,columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created','flair'])\n",
    "\n",
    "for post in india_subreddit.top('week',limit=1000):\n",
    "    if post.title not in posts_pd['title'].unique():\n",
    "         posts.append([post.title, post.score, post.id, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n",
    "posts_pd = pd.DataFrame(posts,columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created','flair'])\n",
    "\n",
    "for post in india_subreddit.hot(limit=1000):\n",
    "    if post.title not in posts_pd['title'].unique():\n",
    "         posts.append([post.title, post.score, post.id, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n",
    "posts_pd = pd.DataFrame(posts,columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created','flair'])\n",
    "\n",
    "for post in india_subreddit.new(limit=1000):\n",
    "    if post.title not in posts_pd['title'].unique():\n",
    "         posts.append([post.title, post.score, post.id, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n",
    "posts_pd = pd.DataFrame(posts,columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created','flair'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list=title_extractor(x_train_pd)\n",
    "all_text2 = ' '.join(title_list)\n",
    "# create a list of words\n",
    "words = all_text2.split()\n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(words)\n",
    "\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten  \n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up a deep neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,  \n",
    "       optimizer=keras.optimizers.Adadelta(),       \n",
    "       metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(prep_x[0:700],y[0:700],epochs=100,validation_data=(prep_x[700:800],y[700:800] )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(prep_x[800:], y[800:], verbose=0)  \n",
    "print('Test loss:', score[0])  \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN,Embedding,LSTM,Dropout\n",
    "from keras.layers import Dense, Dropout, Flatten  \n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "model = Sequential()\n",
    "model.add(LSTM(512,input_shape=(1,200),return_sequences=False))#True = many to many\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256,kernel_initializer='normal',activation='relu'))\n",
    "# model.add(Dense(64,kernel_initializer='normal',activation='linear'))\n",
    "model.add(Dense(32,kernel_initializer='normal',activation='sigmoid'))\n",
    "model.add(Dense(11,kernel_initializer='normal',activation='relu'))\n",
    "model.compile(loss='mse',optimizer ='adam',metrics=['accuracy'])\n",
    "# scores = model.evaluate(train_x[0:5],train_y[0:5],verbose=1,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(p_x[0:700],y[0:700],epochs=100,validation_data=(p_x[700:800],y[700:800] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_ready(data,embed):\n",
    "    t_list=title_extractor(data)\n",
    "    pp_data=preProcessData(t_list)\n",
    "    clean=cleaner(pp_data)\n",
    "    max_len=max_finder(clean)\n",
    "    clean=pad_features(clean,max_len)\n",
    "    final=vectorizer(clean,embed)\n",
    "    final=sent_vectorizer(final)\n",
    "    final=np.array(final)\n",
    "#     re_final=reshaper(final)\n",
    "#     return re_final\n",
    "    return final\n",
    "# train_x=data_to_ready(x_train_pd,embeddings_index)\n",
    "# val_x=data_to_ready(x_val_pd,embeddings_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for flair in posts_pd['flair']:\n",
    "    if flair == 'Unverified' or flair == 'AMA' or flair == 'AMA Concluded':\n",
    "        posts_pd['flair'][i]='AMA'\n",
    "        print(posts_pd['flair'][i])\n",
    "    i=i+1\n",
    "i=0\n",
    "for flair in posts_pd['flair']:\n",
    "    if flair == 'Policy & Economy' or flair == 'Policy' or flair == 'Demonetization' or flair == 'Policy/Economy ':\n",
    "        posts_pd['flair'][i]='Policy/Economy'\n",
    "        print(posts_pd['flair'][i])\n",
    "    i=i+1   \n",
    "i=0\n",
    "for flair in posts_pd['flair']:\n",
    "    if flair == 'Entertainment' or flair == 'Misleading' or flair == 'r/all' or flair == '/r/all':\n",
    "        posts_pd['flair'][i]='Non-Political'\n",
    "        print(posts_pd['flair'][i])\n",
    "    i=i+1    \n",
    "i=0\n",
    "for flair in posts_pd['flair']:\n",
    "    if flair == 'Science & Technology':\n",
    "        posts_pd['flair'][i]='Science/Technology'\n",
    "        print(posts_pd['flair'][i])\n",
    "    i=i+1    \n",
    "    \n",
    "flair_list=['AMA','AskIndia','Business/Finance','Food','Non-Political','Photography','Policy/Economy','Politics','Science/Technology','Sports','[R]eddiquette']\n",
    "i=0\n",
    "for flair in posts_pd['flair']:\n",
    "    if flair not in flair_list:\n",
    "        posts_pd['flair'][i]='waste'\n",
    "        print(posts_pd['flair'][i])\n",
    "    i=i+1  \n",
    "\n",
    "temp=[]\n",
    "for i in range(len(posts_pd)):\n",
    "    if posts_pd['flair'][i]!='waste':\n",
    "        temp.append([posts_pd.title[i], posts_pd.score[i], posts_pd.id[i], posts_pd.url[i], posts_pd.num_comments[i], posts_pd.body[i], posts_pd.created[i],posts_pd.flair[i]])\n",
    "posts_corr= pd.DataFrame(temp,columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created','flair'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = reddit = praw.Reddit(client_id='300HIOocldVcKA', client_secret='PCktLUhpPaRB1RrBCouEDPdpEBU', user_agent='abhishek chopra')\n",
    "\n",
    "subreddit = reddit.subreddit('india')\n",
    "topics_dict = {\"flair\":[], \"title\":[], \"score\":[], \"id\":[], \"url\":[], \"body\":[],\"comms_num\": [], \"created\": [], \"comments\":[]}\n",
    "import datetime as dt\n",
    "def date_convertor(data):\n",
    "    return dt.datetime.fromtimestamp(data)\n",
    "\n",
    "for flair in flair_list:\n",
    "  \n",
    "  get_subreddits = subreddit.search(flair, limit=100)\n",
    "  \n",
    "  for submission in get_subreddits:\n",
    "    \n",
    "    topics_dict[\"flair\"].append(flair)\n",
    "    topics_dict[\"title\"].append(submission.title)\n",
    "    topics_dict[\"score\"].append(submission.score)\n",
    "    topics_dict[\"id\"].append(submission.id)\n",
    "    topics_dict[\"url\"].append(submission.url)\n",
    "    topics_dict[\"body\"].append(submission.selftext)\n",
    "    topics_dict[\"comms_num\"].append(submission.num_comments)\n",
    "    topics_dict[\"created\"].append(submission.created)\n",
    "    \n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comment = ''\n",
    "    for top_level_comment in submission.comments:\n",
    "        comment = comment + ' ' + top_level_comment.body\n",
    "    topics_dict[\"comments\"].append(comment)\n",
    "    \n",
    "topics_data = pd.DataFrame(topics_dict)\n",
    "_timestamp = topics_data[\"created\"].apply(date_convertor)\n",
    "topics_data = topics_data.assign(timestamp = _timestamp)\n",
    "del topics_data['created']\n",
    "topics_data.to_csv('reddit-india-data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extractor(text,text_type):\n",
    "    title_list=[]\n",
    "    for i in range(len(text)):\n",
    "        title_list.append(text[text_type][i])\n",
    "    return title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessData(dataa):\n",
    "    stopwords_en = list(set(stopwords.words('english')))\n",
    "    def split(word): \n",
    "        return [char for char in word]   \n",
    "    punchList = split(punctuation)\n",
    "\n",
    "    print(stopwords_en)\n",
    "    print('Punctuation :', punchList)\n",
    "\n",
    "    wordTokenList = [word_tokenize(sent) for sent in dataa]\n",
    "    lowercasingList = [[word.lower() for word in sentence] for sentence in wordTokenList]\n",
    "    noStopWordList = [[word for word in sentence if word not in stopwords_en] for sentence in lowercasingList]\n",
    "    noPunchList = [[re.sub(r'([^\\s\\w]|_)+', '', word) for word in sentence] for sentence in noStopWordList]\n",
    "    #noPunchList = [[word for word in sentence if word not in punchList] for sentence in noStopWordList]\n",
    "    PP_data = [[word for word in sentence if word] for sentence in noPunchList]\n",
    "    return PP_data\n",
    "\n",
    "# PP_data = preProcessData(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "glove_dir = 'glove'\n",
    "embeddings_index = {} # empty dictionary\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "z=np.zeros(200)\n",
    "embeddings_index['ignore_nan']=z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(PP_data):\n",
    "    PP_corr=[]\n",
    "    for title in PP_data:\n",
    "        temp=[]\n",
    "        for word in title:\n",
    "            if word in embeddings_index.keys():\n",
    "                    temp.append(word)\n",
    "        PP_corr.append(temp)\n",
    "    return PP_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_finder(PP_corr):\n",
    "    max_len=len(PP_corr[0])\n",
    "    for i in range(len(PP_corr)):\n",
    "        if max_len<len(PP_corr[i]):\n",
    "            max_len=len(PP_corr[i])\n",
    "    return max_len    \n",
    "\n",
    "def pad_features(titles,max_len):\n",
    "    features=[]\n",
    "    for title in titles:\n",
    "        temp=[]\n",
    "        for i in range(max_len):\n",
    "            if i < len(title):\n",
    "                temp.append(title[i])\n",
    "            else:\n",
    "                temp.append(\"ignore_nan\")\n",
    "        features.append(temp)\n",
    "    return features\n",
    "\n",
    "\n",
    "def vectorizer(summ_data,embeddings_index):\n",
    "    summ_vec=[]\n",
    "    for list_w in summ_data:\n",
    "        temp=[]\n",
    "        for word in list_w:\n",
    "            if word in embeddings_index.keys():\n",
    "                temp.append(embeddings_index[word])\n",
    "        summ_vec.append(temp)\n",
    "    return summ_vec\n",
    "\n",
    "def sent_vectorizer(vector):\n",
    "    sent_vect=[]\n",
    "    for sent in vector:\n",
    "        n=0\n",
    "        temp=np.zeros(200)\n",
    "        for word in sent:\n",
    "            temp=temp+word\n",
    "            n=n+1\n",
    "        sent_vect.append(temp/n)\n",
    "    return sent_vect\n",
    "\n",
    "def reshaper(t_x):\n",
    "    x = np.reshape(t_x, (t_x.shape[0], 1,t_x.shape[1]))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def shuffler(data):\n",
    "    df_sh=shuffle(data)\n",
    "    df_sh=df_sh.reset_index()\n",
    "    return df_sh\n",
    "\n",
    "def one_shotter(z,colt):\n",
    "    y=z[colt]\n",
    "    y=np.array(y)\n",
    "    y=pd.get_dummies(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['too', 'were', 'on', 'during', \"that'll\", 'have', \"wouldn't\", 'there', 'off', 'her', 'few', 'but', 'its', 'won', \"won't\", 'than', 'didn', 'ain', \"she's\", 'is', 'if', 'themselves', 'been', 'has', 'being', 'weren', 'between', 'be', 'ourselves', 'doesn', 'should', 'same', 'any', 'up', 'just', 'i', 'am', 'ma', 'about', 'when', 'such', 't', 'couldn', \"mustn't\", 'wasn', \"you're\", 'for', 'then', 'once', 's', 'to', 'yourself', \"don't\", 'having', \"aren't\", 'them', 'at', 'do', 'their', 'hadn', 'so', \"you've\", 'it', 'a', 'while', 'below', 'why', 'an', 'each', \"you'd\", 'down', 'this', 'that', 'through', \"you'll\", 'here', 'who', 'against', 'until', 'mightn', 'how', 'above', 'don', 'in', 'wouldn', 'had', 'not', 'isn', 'what', 'very', 'are', 'both', 'hasn', 'out', 'they', 'd', \"hasn't\", 'needn', 'with', 'she', 'haven', 'under', 'mustn', 'where', 'some', 'the', 'itself', 'm', 'shouldn', 'you', 'y', 'his', 'our', \"hadn't\", 'as', 'he', 'my', 'aren', \"weren't\", 'myself', 'by', 'shan', 'into', 've', 'over', 'yourselves', 'only', \"couldn't\", 'was', 'these', 'most', 'which', 'herself', 'further', \"doesn't\", 'or', 'own', \"haven't\", 'yours', 'o', 'we', 'all', 'can', 'will', \"shouldn't\", 'and', 'more', \"wasn't\", 're', \"it's\", \"needn't\", \"shan't\", 'does', \"should've\", 'him', 'me', 'again', 'before', 'your', \"mightn't\", 'himself', 'did', 'those', 'hers', 'll', 'nor', 'after', 'doing', \"isn't\", 'whom', 'now', 'theirs', 'no', 'of', 'from', 'other', 'because', 'ours', \"didn't\"]\n",
      "Punctuation : ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "def joiner(data):\n",
    "    input_corrected = [\" \".join(i) for i in data]\n",
    "    return input_corrected\n",
    "def data_prep(data,text_type):\n",
    "    sh=shuffler(data)\n",
    "    y=sh['flair']\n",
    "    x=joiner(preProcessData(text_extractor(sh,text_type)))\n",
    "    return x,y\n",
    "x,y=data_prep(topics_data,'title')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforest(x_train, x_test, y_train, y_test):\n",
    "    model = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', RandomForestClassifier(n_estimators = 900)),\n",
    "                 ])\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    filename = 'rf_model.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "    print(classification_report(y_test, y_pred,target_names=flairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_svm(x_train, x_test, y_train, y_test):  \n",
    "    model= Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-2, max_iter=10, tol=None)),\n",
    "                 ])\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    filename = 'lsvm_model.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "    print(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logisticreg(x_train, x_test, y_train, y_test):\n",
    "    model = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "                 ])\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    filename = 'lg_model.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "    print(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5970149253731343\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "               AMA       0.73      0.44      0.55        25\n",
      "          AskIndia       0.70      0.61      0.65        23\n",
      "  Business/Finance       0.53      0.32      0.40        31\n",
      "              Food       0.96      0.80      0.87        30\n",
      "     Non-Political       0.83      0.94      0.88        31\n",
      "       Photography       0.67      0.72      0.69        25\n",
      "    Policy/Economy       0.52      0.65      0.58        23\n",
      "          Politics       0.24      0.58      0.34        24\n",
      "Science/Technology       0.57      0.52      0.54        25\n",
      "            Sports       0.75      0.48      0.59        25\n",
      "     [R]eddiquette       0.00      0.00      0.00         6\n",
      "\n",
      "          accuracy                           0.60       268\n",
      "         macro avg       0.59      0.55      0.55       268\n",
      "      weighted avg       0.64      0.60      0.60       268\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anacoda\\envs\\machine\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "randomforest(x[0:750],x[750:],y[0:750], y[750:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6305970149253731\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "               AMA       0.62      0.52      0.57        25\n",
      "          AskIndia       0.59      0.74      0.65        23\n",
      "  Business/Finance       0.69      0.29      0.41        31\n",
      "              Food       0.77      0.80      0.79        30\n",
      "     Non-Political       0.70      0.97      0.81        31\n",
      "       Photography       0.71      0.80      0.75        25\n",
      "    Policy/Economy       0.49      0.74      0.59        23\n",
      "          Politics       0.42      0.42      0.42        24\n",
      "Science/Technology       0.61      0.68      0.64        25\n",
      "            Sports       0.80      0.48      0.60        25\n",
      "     [R]eddiquette       0.00      0.00      0.00         6\n",
      "\n",
      "          accuracy                           0.63       268\n",
      "         macro avg       0.58      0.58      0.57       268\n",
      "      weighted avg       0.63      0.63      0.61       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linear_svm(x[0:750],x[750:],y[0:750], y[750:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6082089552238806\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "               AMA       0.67      0.48      0.56        25\n",
      "          AskIndia       0.50      0.61      0.55        23\n",
      "  Business/Finance       0.50      0.32      0.39        31\n",
      "              Food       0.83      0.80      0.81        30\n",
      "     Non-Political       0.83      0.94      0.88        31\n",
      "       Photography       0.74      0.80      0.77        25\n",
      "    Policy/Economy       0.44      0.65      0.53        23\n",
      "          Politics       0.46      0.50      0.48        24\n",
      "Science/Technology       0.52      0.60      0.56        25\n",
      "            Sports       0.63      0.48      0.55        25\n",
      "     [R]eddiquette       0.00      0.00      0.00         6\n",
      "\n",
      "          accuracy                           0.61       268\n",
      "         macro avg       0.56      0.56      0.55       268\n",
      "      weighted avg       0.61      0.61      0.60       268\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anacoda\\envs\\machine\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "d:\\anacoda\\envs\\machine\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logisticreg(x[0:750],x[750:],y[0:750], y[750:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
