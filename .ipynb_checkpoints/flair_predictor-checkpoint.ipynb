{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Abhishek's\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abhishek's\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#downloading libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "#nltk.download()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'rf_model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pprint\n",
    "reddit = praw.Reddit(client_id='300HIOocldVcKA', client_secret='PCktLUhpPaRB1RrBCouEDPdpEBU', user_agent='abhishek chopra') # potentially needs configuring, see docs\n",
    "\n",
    "\n",
    "# post = reddit.submission(url=solved_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessData(dataa):\n",
    "    stopwords_en = list(set(stopwords.words('english')))\n",
    "    def split(word): \n",
    "        return [char for char in word]   \n",
    "    punchList = split(punctuation)\n",
    "\n",
    "    print(stopwords_en)\n",
    "    print('Punctuation :', punchList)\n",
    "\n",
    "    wordTokenList = [word_tokenize(sent) for sent in dataa]\n",
    "    lowercasingList = [[word.lower() for word in sentence] for sentence in wordTokenList]\n",
    "    noStopWordList = [[word for word in sentence if word not in stopwords_en] for sentence in lowercasingList]\n",
    "    noPunchList = [[re.sub(r'([^\\s\\w]|_)+', '', word) for word in sentence] for sentence in noStopWordList]\n",
    "    #noPunchList = [[word for word in sentence if word not in punchList] for sentence in noStopWordList]\n",
    "    PP_data = [[word for word in sentence if word] for sentence in noPunchList]\n",
    "    return PP_data\n",
    "\n",
    "def text_extractor(text,text_type):\n",
    "    title_list=[]\n",
    "    for i in range(len(text)):\n",
    "        title_list.append(text[text_type][i])\n",
    "    return title_list\n",
    "def joiner(data):\n",
    "    input_corrected = [\" \".join(i) for i in data]\n",
    "    return input_corrected\n",
    "# x=joiner(preProcessData(text_extractor(sh,text_type)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_flair(url,loaded_model):\n",
    "\n",
    "    submission = reddit.submission(url=url)\n",
    "    topics_dict = {\"title\":[], \"comments\":[]}\n",
    "    topics_dict[\"title\"].append(submission.title)\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comment = ''\n",
    "    for top_level_comment in submission.comments:\n",
    "        comment = comment + ' ' + top_level_comment.body\n",
    "    topics_dict[\"comments\"].append(comment)\n",
    "    \n",
    "    topics_data = pd.DataFrame(topics_dict)\n",
    "    feature_combine = topics_data[\"title\"] + topics_data[\"comments\"]\n",
    "    topics_data = topics_data.assign(feature_combine = feature_combine)\n",
    "    feature=text_extractor(topics_data,'feature_combine')\n",
    "    x=joiner(preProcessData(feature))\n",
    "    return loaded_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['now', 'needn', 'y', 'who', 'what', 'but', 'both', 'o', 'why', 'you', 'here', 'mustn', 'some', 'during', 'out', 'or', \"aren't\", 'don', 'the', 'once', 'it', 'there', 'very', 'have', 'hers', \"shouldn't\", 'wasn', 'your', 'again', 'was', 'when', 'himself', 'ourselves', \"didn't\", 'same', 'his', 'which', 'will', 'hadn', 'as', 'an', 'theirs', 'under', 'didn', 'he', 'this', 's', 'through', 'over', 'shouldn', 'in', 'only', 'be', \"you'd\", \"she's\", 'll', \"mightn't\", 'below', 'and', 'where', \"won't\", 'hasn', \"wasn't\", 'are', 'so', 'themselves', 'with', \"should've\", 'weren', 'at', 'do', 'against', 'off', \"haven't\", 'because', 're', \"hasn't\", 'their', 'had', 'yourselves', 'she', \"wouldn't\", 'above', \"couldn't\", 'further', 'herself', \"you've\", 'has', 'isn', 'wouldn', 'should', 'no', 'between', 'until', 'were', 'any', 'most', 'before', 'these', 'down', 'more', 'm', 'mightn', 'after', 'not', 'does', 'ma', 'doesn', 'been', 'they', 'aren', 'being', 'while', 'having', 'for', 'than', 'them', 'can', 'my', 'just', 'our', 'him', 'ain', 'couldn', \"shan't\", 'to', 't', 'each', \"hadn't\", 'myself', 'a', 'all', 'd', 'how', 'won', \"it's\", 'her', 'doing', \"doesn't\", 'on', 've', 'yourself', 'up', \"that'll\", 'am', \"needn't\", 'that', 'whom', 'yours', 'we', 'its', 'did', 'by', 'ours', 'into', \"don't\", \"weren't\", 'own', 'if', 'shan', 'about', 'from', 'too', 'then', 'i', 'few', \"you're\", 'me', \"isn't\", 'other', 'those', \"you'll\", 'nor', \"mustn't\", 'such', 'is', 'haven', 'itself', 'of']\n",
      "Punctuation : ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Politics'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_flair('https://www.reddit.com/r/india/comments/cyj7gz/kashmir_news_coverage_3/https://github.com/akshaybhatia10/Reddit-Flair-Detection.git',loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
